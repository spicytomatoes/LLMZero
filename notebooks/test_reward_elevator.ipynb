{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ianch\\miniconda3\\envs\\aiplanning\\Lib\\site-packages\\pyRDDLGym\\Examples c:\\Users\\ianch\\miniconda3\\envs\\aiplanning\\Lib\\site-packages\\pyRDDLGym\\Examples\\manifest.csv\n",
      "Available example environment(s):\n",
      "CartPole_continuous -> A simple continuous state-action MDP for the classical cart-pole system by Rich Sutton, with actions that describe the continuous force applied to the cart.\n",
      "CartPole_discrete -> A simple continuous state MDP for the classical cart-pole system by Rich Sutton, with discrete actions that apply a constant force on either the left or right side of the cart.\n",
      "Elevators -> The Elevator domain models evening rush hours when people from different floors in a building want to go down to the bottom floor using elevators.\n",
      "HVAC -> Multi-zone and multi-heater HVAC control problem\n",
      "MarsRover -> Multi Rover Navigation, where a group of agent needs to harvest mineral.\n",
      "MountainCar -> A simple continuous MDP for the classical mountain car control problem.\n",
      "NewLanguage -> Example with new language features.\n",
      "NewtonZero -> Example with Newton root-finding method.\n",
      "PowerGen_continuous -> A continuous simple power generation problem loosely modeled on the problem of unit commitment.\n",
      "PowerGen_discrete -> A simple power generation problem loosely modeled on the problem of unit commitment.\n",
      "PropDBN -> Simple propositional DBN.\n",
      "RaceCar -> A simple continuous MDP for the racecar problem.\n",
      "RecSim -> A problem of recommendation systems, with consumers and providers.\n",
      "Reservoir_continuous -> Continuous action version of management of the water level in interconnected reservoirs.\n",
      "Reservoir_discrete -> Discrete version of management of the water level in interconnected reservoirs.\n",
      "SupplyChain -> A supply chain with factory and multiple warehouses.\n",
      "SupplyChainNet -> A supply chain network with factory and multiple warehouses.\n",
      "Traffic -> BLX/QTM traffic model.\n",
      "UAV_continuous -> Continuous action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
      "UAV_discrete -> Discrete action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
      "UAV_mixed -> Mixed action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
      "Wildfire -> A boolean version of the wildfire fighting domain.\n",
      "The building has 5 floors and 1 elevators. Each floor has maximum 3 people waiting. Each elevator can carry maximum of 10 people.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ianch\\miniconda3\\envs\\aiplanning\\Lib\\site-packages\\pyRDDLGym\\Core\\Env\\RDDLConstraints.py:85: UserWarning: Constraint does not have a structure of <action or state fluent> <op> <rhs>, where:\n",
      "<op> is one of {<=, <, >=, >}\n",
      "<rhs> is a deterministic function of non-fluents or constants only.\n",
      ">> ( sum_{?f: floor} [ elevator-at-floor(?e, ?f) ] ) == 1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from agents.elevator_expert import ElevatorExpertPolicyAgent\n",
    "from agents.random_agent import RandomAgent\n",
    "from agents.llmzero import LLMTransitionModel, LLMRewardModel\n",
    "from environments.ElevatorEnvironment import ElevatorEnvironment\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "env = ElevatorEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmzero_reward_model = LLMRewardModel(\n",
    "    debug=False,\n",
    "    env_params={\n",
    "                    \"system_prompt_path\": \"../prompts/prompt_elevator_reward.txt\",\n",
    "                    \"extract_reward_regex\": r\"TOTAL_REWARD_FINAL = (.*)\\n\", # only use the first match, same line\n",
    "                    \"extract_reward_regex_fallback\": [r\"TOTAL_REWARD_FINAL = (.*)\\n\"],\n",
    "                    \"extract_done_regex\": r\"done: (.*)\",\n",
    "                    \"extract_done_regex_fallback\": [r\"done: (.*)\"],\n",
    "                },\n",
    "    load_prompt_buffer_path=\"../prompt_buffer/elevator_reward_20241110_014634.pkl\",\n",
    "    prompt_buffer_prefix=\"../prompt_buffer/elevator_reward\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 117\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = RandomAgent(env, seed=SEED)\n",
    "expert_agent = ElevatorExpertPolicyAgent()\n",
    "\n",
    "# random agent\n",
    "state, _ = env.reset(SEED)\n",
    "done = False\n",
    "\n",
    "random_agent_trajectory = []\n",
    "\n",
    "while not done:\n",
    "    action = random_agent.act(state)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    random_agent_trajectory.append((state, action, reward, next_state, done))\n",
    "    state = next_state\n",
    "    \n",
    "# expert agent\n",
    "state, _ = env.reset(SEED + 1)  # use a different seed for the expert agent\n",
    "done = False\n",
    "\n",
    "expert_agent_trajectory = []\n",
    "\n",
    "while not done:\n",
    "    action = expert_agent.act(state)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    expert_agent_trajectory.append((state, action, reward, next_state, done))\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories_combined = random_agent_trajectory + expert_agent_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mistral-large-2407'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmzero_reward_model.llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def test_reward_model(model, trajectories):\n",
    "    gt_rewards = []\n",
    "    predicted_rewards = []\n",
    "    squared_errors = []\n",
    "    status_list = []\n",
    "    \n",
    "    pbar = tqdm.tqdm(trajectories)\n",
    "    \n",
    "    for trajectory in pbar:\n",
    "        state, action, reward, next_state, done = trajectory\n",
    "        \n",
    "        state_text = env.state_to_text(state)\n",
    "        action_text = env.action_to_text(action)\n",
    "        \n",
    "        predicted_reward, status = model.get_reward(state_text, action_text)\n",
    "        \n",
    "        gt_rewards.append(reward)\n",
    "        predicted_rewards.append(predicted_reward)\n",
    "        squared_errors.append((reward - predicted_reward) ** 2 / (reward + 1.0))\n",
    "        status_list.append(status)\n",
    "        \n",
    "        pbar.set_description(f\"Squared error: {np.mean(squared_errors)}\")\n",
    "        \n",
    "    return gt_rewards, predicted_rewards, status_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squared error: -93.54598436216806:  68%|██████▊   | 274/400 [01:01<00:48,  2.58it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prompt buffer to ../prompt_buffer/elevator_reward_20241110_014634.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squared error: -93.1799607114933:  71%|███████▏  | 285/400 [02:42<09:42,  5.06s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prompt buffer to ../prompt_buffer/elevator_reward_20241110_014634.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squared error: -92.04032326695396:  74%|███████▍  | 295/400 [05:09<34:55, 19.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prompt buffer to ../prompt_buffer/elevator_reward_20241110_014634.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squared error: -91.46837960003316:  76%|███████▋  | 305/400 [06:56<17:24, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prompt buffer to ../prompt_buffer/elevator_reward_20241110_014634.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squared error: -90.20826998083032:  79%|███████▉  | 315/400 [08:18<11:06,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prompt buffer to ../prompt_buffer/elevator_reward_20241110_014634.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squared error: -88.7088428629399:  81%|████████▏ | 325/400 [10:09<13:04, 10.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prompt buffer to ../prompt_buffer/elevator_reward_20241110_014634.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squared error: -87.32342350197447:  84%|████████▍ | 335/400 [12:34<12:02, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prompt buffer to ../prompt_buffer/elevator_reward_20241110_014634.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squared error: -86.19945649444837:  86%|████████▋ | 345/400 [15:41<13:43, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prompt buffer to ../prompt_buffer/elevator_reward_20241110_014634.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squared error: -85.47329849744098:  89%|████████▉ | 355/400 [17:33<10:20, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prompt buffer to ../prompt_buffer/elevator_reward_20241110_014634.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squared error: -86.06216330469893:  91%|█████████▏| 365/400 [19:12<05:41,  9.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prompt buffer to ../prompt_buffer/elevator_reward_20241110_014634.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Squared error: -85.26172570916157:  93%|█████████▎| 371/400 [20:32<01:36,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No match found with fallback regex, using full response as reward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gt_rewards, predicted_rewards, status_list \u001b[38;5;241m=\u001b[39m \u001b[43mtest_reward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllmzero_reward_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrajectories_combined\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 21\u001b[0m, in \u001b[0;36mtest_reward_model\u001b[1;34m(model, trajectories)\u001b[0m\n\u001b[0;32m     19\u001b[0m gt_rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[0;32m     20\u001b[0m predicted_rewards\u001b[38;5;241m.\u001b[39mappend(predicted_reward)\n\u001b[1;32m---> 21\u001b[0m squared_errors\u001b[38;5;241m.\u001b[39mappend((\u001b[43mreward\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpredicted_reward\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m (reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m))\n\u001b[0;32m     22\u001b[0m status_list\u001b[38;5;241m.\u001b[39mappend(status)\n\u001b[0;32m     24\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSquared error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(squared_errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "gt_rewards, predicted_rewards, status_list = test_reward_model(llmzero_reward_model, trajectories_combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiplanning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
