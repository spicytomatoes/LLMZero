{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ianch\\miniconda3\\envs\\aiplanning\\Lib\\site-packages\\pyRDDLGym\\Examples c:\\Users\\ianch\\miniconda3\\envs\\aiplanning\\Lib\\site-packages\\pyRDDLGym\\Examples\\manifest.csv\n",
      "Available example environment(s):\n",
      "CartPole_continuous -> A simple continuous state-action MDP for the classical cart-pole system by Rich Sutton, with actions that describe the continuous force applied to the cart.\n",
      "CartPole_discrete -> A simple continuous state MDP for the classical cart-pole system by Rich Sutton, with discrete actions that apply a constant force on either the left or right side of the cart.\n",
      "Elevators -> The Elevator domain models evening rush hours when people from different floors in a building want to go down to the bottom floor using elevators.\n",
      "HVAC -> Multi-zone and multi-heater HVAC control problem\n",
      "MarsRover -> Multi Rover Navigation, where a group of agent needs to harvest mineral.\n",
      "MountainCar -> A simple continuous MDP for the classical mountain car control problem.\n",
      "NewLanguage -> Example with new language features.\n",
      "NewtonZero -> Example with Newton root-finding method.\n",
      "PowerGen_continuous -> A continuous simple power generation problem loosely modeled on the problem of unit commitment.\n",
      "PowerGen_discrete -> A simple power generation problem loosely modeled on the problem of unit commitment.\n",
      "PropDBN -> Simple propositional DBN.\n",
      "RaceCar -> A simple continuous MDP for the racecar problem.\n",
      "RecSim -> A problem of recommendation systems, with consumers and providers.\n",
      "Reservoir_continuous -> Continuous action version of management of the water level in interconnected reservoirs.\n",
      "Reservoir_discrete -> Discrete version of management of the water level in interconnected reservoirs.\n",
      "SupplyChain -> A supply chain with factory and multiple warehouses.\n",
      "SupplyChainNet -> A supply chain network with factory and multiple warehouses.\n",
      "Traffic -> BLX/QTM traffic model.\n",
      "UAV_continuous -> Continuous action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
      "UAV_discrete -> Discrete action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
      "UAV_mixed -> Mixed action space version of multi-UAV problem where a group of UAVs have to reach goal positions in the 3d Space.\n",
      "Wildfire -> A boolean version of the wildfire fighting domain.\n",
      "The building has 5 floors and 1 elevators. Each floor has maximum 3 people waiting. Each elevator can carry maximum of 10 people.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ianch\\miniconda3\\envs\\aiplanning\\Lib\\site-packages\\pyRDDLGym\\Core\\Env\\RDDLConstraints.py:85: UserWarning: Constraint does not have a structure of <action or state fluent> <op> <rhs>, where:\n",
      "<op> is one of {<=, <, >=, >}\n",
      "<rhs> is a deterministic function of non-fluents or constants only.\n",
      ">> ( sum_{?f: floor} [ elevator-at-floor(?e, ?f) ] ) == 1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from agents.elevator_expert import ElevatorExpertPolicyAgent\n",
    "from agents.random_agent import RandomAgent\n",
    "from agents.llmzero import LLMTransitionModel, LLMRewardModel\n",
    "from environments.ElevatorEnvironment import ElevatorEnvironment\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "env = ElevatorEnvironment()\n",
    "\n",
    "llmzero_reward_model = LLMRewardModel(\n",
    "    debug=True,\n",
    "    env_params={\n",
    "        \"system_prompt_path\": \"../prompts/prompt_elevator_reward.txt\",\n",
    "        \"extract_reward_regex\": r\"TOTAL_REWARD_FINAL = (.*)\\n\", # only use the first match, same line\n",
    "        \"extract_reward_regex_fallback\": [r\"TOTAL_REWARD_FINAL = (.*)\\n\"],\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 117\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = RandomAgent(env, seed=SEED)\n",
    "expert_agent = ElevatorExpertPolicyAgent()\n",
    "\n",
    "# random agent\n",
    "state, _ = env.reset(SEED)\n",
    "done = False\n",
    "\n",
    "random_agent_trajectory = []\n",
    "\n",
    "while not done:\n",
    "    action = random_agent.act(state)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    random_agent_trajectory.append((state, action, reward, next_state, done))\n",
    "    state = next_state\n",
    "    \n",
    "# expert agent\n",
    "state, _ = env.reset(SEED + 1)  # use a different seed for the expert agent\n",
    "done = False\n",
    "\n",
    "expert_agent_trajectory = []\n",
    "\n",
    "while not done:\n",
    "    action = expert_agent.act(state)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    expert_agent_trajectory.append((state, action, reward, next_state, done))\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories_combined = random_agent_trajectory + expert_agent_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'open-mixtral-8x22b'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmzero_reward_model.llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward_model(model, trajectories):\n",
    "    gt_rewards = []\n",
    "    predicted_rewards = []\n",
    "    \n",
    "    for trajectory in trajectories:\n",
    "        state, action, reward, next_state, done = trajectory\n",
    "        \n",
    "        state_text = env.state_to_text(state)\n",
    "        \n",
    "        predicted_reward = model.get_reward(state_text)\n",
    "        \n",
    "        gt_rewards.append(reward)\n",
    "        predicted_rewards.append(predicted_reward)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiplanning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
